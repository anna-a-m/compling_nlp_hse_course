{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61c933c0",
   "metadata": {},
   "source": [
    "# Reinforcement learning from (human) feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc96f43c",
   "metadata": {},
   "source": [
    "В этом семинаре я постараюсь поверхностно рассказать про обучение с подкреплением (Reinforcement learning) с обратной связью от человека (human feedback). Поверхносто это будет из-за того, что это новый метод, который к тому же не до конца публичный. RLHF был разработан OpenAI и именно он позволил достичь сильного прироста в качестве генеративных моделей, которое можно наблюдать в ChatGPT. OpenAI не раскрывают всех деталей того, как они этого добились. Есть только обобщенные описание того, как это все работает и большая часть научного/инженерного сообщества в NLP сейчас пытается возпроизвести RLHF. \n",
    "\n",
    "В семинаре мы попробуем использовать библиотеку TRL - https://github.com/lvwerra/trl \n",
    "Это библиотека связана с Huggingface и поэтому ей удобно пользоваться. (Еще есть похожая библиотека - https://github.com/CarperAI/trlx, я не до конца понимаю насколько она связана с trl, кажется что обе развиваются) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8a2af05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers trl wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca08c3d",
   "metadata": {},
   "source": [
    "### Reinforcement learning \n",
    "Прежде чем разбирать RLHF давайте коротко разберем, что такое Reinforcement learning в целом. RL - это отдельная парадигма в машинном обучении наряду с supervised и unsupervised learning. RL похоже на supervised learning, но модель в нем обучается не на конкретных примерах, а используя положительную/отрицательную обратную связь на свои действия от среды. \n",
    "Например, RL применяется в играх (от шахмат до доты) и в робототехнике. В принципе можно обучить supervised модель на действиях людей в игре или в управлении роботом, но тогда получается, что модель будет учиться имитировать людей, а не искать оптимальное решение. В этих задачах можно обойтись и без людей, так как можно автоматически оценить успешность/не успешность - в игре это очки или победа, в роботике это может быть, например, перемещение предмета из точки а в точку б. Обобщоно RL можно представить вот так:\n",
    "\n",
    "![](https://www.guru99.com/images/1/082319_0514_Reinforceme1.png)\n",
    "\n",
    "Модель учиться предсказывать действия в конкретной ситуации так, чтобы в итоге прийти к успеху (в игре действия могут быть движение вниз или вверх, а конкретной ситуацией может быть текущий экран). Изначально модель совершает рандомные действия, но постепенно у нее накапливаются последовательности действий, которые ведут к успеху и она начинает их повторять (и избегать тех, которые ведут к поражению).\n",
    "На практике это может быть очень сложно, так как часто в играх количество потенциально возможных последовательностей очень большое и модель просто не может случайно найти правильные последовательности,и требуется использовать какие-то трюки, специфичные для задачи (например, можно обучать модели играть друг против друга, чтобы можно было играть много игр парралельно в ускоренном режиме и не зависеть от людей). Сейчас RL это по большей части исследовательская область и есть даже мнения, что в итоге RL окажется не нужен (например, так считает Le Cun). Но в некоторых конкретных задачах, RL получилось успешно применить. В том числе, кажется, успешен он и для улучшение языковых моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b86d1bf",
   "metadata": {},
   "source": [
    "### Как можно применить RL к языковым моделям?\n",
    "При генерации текста у нас нет среды, которая могла бы оценить качество текста. Точнее она есть (это человек), но использовать ее напрямую затруднительно, поэтому в RLHF создается отедльная модель, которая будет имитировать человека и оценивать сгенерированные тексты. Такая модель обучается на датасете, который размечается людьми. Сначала с помощью языковой модели генерируется какое-то количество текстов и людей просят выбрать лучшие из них, чтобы сопоставить каждому тексту какое-то число (усредненная оценка). Затем обучается модель, которая предсказывает такие оценки. И на финальном шаге с RL обучаемая генеративная модель генерирует текст и он оценивается с помощью модели обученной на обратной связи от людей. Модель дообучается так, чтобы генерированные тексты получали высокие оценки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8b9794",
   "metadata": {},
   "source": [
    "![](https://pbs.twimg.com/media/FfeTNJCUcAEVuo5.jpg:large)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef207f9d",
   "metadata": {},
   "source": [
    "В RLHF в реализации от OpenAI есть еще 1 важный этап - это PPO. Это метод, который используется внутри RL для дообучения модели. Важно про него понимать то, что модель не просто учится максимизировать оценку генерированных текстов, она при этом еще старается минимизировать изменения. В простом fine-tuning частая проблема это то, что дообучаемая модель со временем начинает забывать и теряет в качестве, PPO позволяет этого избежать."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70de44c9",
   "metadata": {},
   "source": [
    "### TRL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a2247",
   "metadata": {},
   "source": [
    "Обратная связь от людей в таком фреймворке не обязательна, главное как-то получить модель, которая будет оценивать генерируемые тексты. Можно попытаться использовать доступные модели или датасеты. Давайте попробуем использовать библиотеку trl и дообучим модель OPT-125m генерировать менее негативные тексты использую предобученную модель оценки тональности."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a16d55",
   "metadata": {},
   "source": [
    "Схематично обучение в TRL устроено вот так:\n",
    "\n",
    "1) Rollout - это просто генерация текстов, используя исходную языковую модель. Чтобы тексты были релеватными можно взять определенные промты и генерировать их продолжения.\n",
    "\n",
    "2) Evaluation - это оценивание сгенерированных текстов, в нашем случае мы будет это делать с помощью предобученного классификатора тональности \n",
    "\n",
    "3) Optimization - это само обучение. Обратите внимание, что на этом шаге есть active_model и reference_model. Изначально это одна и та же модель, но в процессе обучения мы изменяем active_model так, чтобы она не сильно уходила от изначальное модели (reference_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4b608b",
   "metadata": {},
   "source": [
    "![](https://camo.githubusercontent.com/85d00cf9bca67e33c2d1270b51ff1ac01853b26a8d6bb226b711f859d065b4a6/68747470733a2f2f68756767696e67666163652e636f2f64617461736574732f74726c2d696e7465726e616c2d74657374696e672f6578616d706c652d696d616765732f7265736f6c76652f6d61696e2f696d616765732f74726c5f6f766572766965772e706e67)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2855c91",
   "metadata": {},
   "source": [
    "Код взят из примеров в самой библиотеке и он на torch, но опять же тут ничего сложного (huggingface, токенизация)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db70187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb7162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f23aefbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# тут задаются гиперпараметры\n",
    "config = PPOConfig(\n",
    "    model_name=\"facebook/opt-125m\",\n",
    "    learning_rate=1.41e-5,\n",
    "    log_with=None,\n",
    "    mini_batch_size=16\n",
    ")\n",
    "\n",
    "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 16}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e237f0e",
   "metadata": {},
   "source": [
    "Для генерирования текстов тут используется IMDB review датасет. От каждого текста отрезается случайно какой-то кусочек и модель должна его продолжить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e7d55ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_dataset(config, dataset_name=\"imdb\", input_min_text_length=2, input_max_text_length=8):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    ds = load_dataset(dataset_name, split=\"train\")\n",
    "    ds = ds.rename_columns({\"text\": \"review\"})\n",
    "    \n",
    "    # еще есть фильтрация по длине\n",
    "    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
    "    ds = ds.filter(lambda x: len(x[\"review\"]) < 2000, batched=False)\n",
    "    \n",
    "    # длина кусочка определяется случайно\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f867903c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-cd96d6dda9f74d63.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-d1953048d0486394.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-0b60bdf639a5236d.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = build_dataset(config)\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc054ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_model\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "# reference_model (обратите внимание что это одна и так же модель изначально)\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c2a4b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f37f64b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833df8bb",
   "metadata": {},
   "source": [
    "Для оценки тональности используется пайплайн из huggingface и предобученная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59495c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"lvwerra/distilbert-imdb\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a19fdf",
   "metadata": {},
   "source": [
    "Для каждого текста она выдает скор негативности и положительности. Для обучения дальше будет использоваться скор положительности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae2f1522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'NEGATIVE', 'score': 2.3350486755371094},\n",
       "  {'label': 'POSITIVE', 'score': -2.726576566696167}]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"this movie was really bad!!\"\n",
    "sentiment_pipe(text, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2e56759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'NEGATIVE', 'score': -2.2947897911071777},\n",
       "  {'label': 'POSITIVE', 'score': 2.557039737701416}]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"this movie was really good!!\"\n",
    "sentiment_pipe(text, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6b6551",
   "metadata": {},
   "source": [
    "Еще одно место, где можно настраивать параметры. Эти параметры мы разбирали на семинаре по GPT они отвечают за генерацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3cdf60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {\"min_length\": -1, \"top_k\": 0.0, \"top_p\": 1.0, \n",
    "              \"do_sample\": True, \"pad_token_id\": tokenizer.eos_token_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42757f1",
   "metadata": {},
   "source": [
    "Само обучение. Для небольших моделей оно даже не очень долгое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75e56fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c9384f23ba4928916ab989ea11a668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output_min_length = 4\n",
    "output_max_length = 16\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "\n",
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader), \n",
    "                         total=dataset.num_rows//ppo_trainer.dataloader.batch_sampler.batch_size):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    #### Get response from gpt2\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        gen_len = output_length_sampler()\n",
    "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "        response_tensors.append(response.squeeze()[-gen_len:])\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    #### Compute sentiment score\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
    "    rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "\n",
    "    #### Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44dd2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 3000)\n",
    "pd.set_option('display.max_colwidth', 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c62daf",
   "metadata": {},
   "source": [
    "Можно посмотреть на результаты, сгенериров продолжения старой и новой модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f251f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response (before)</th>\n",
       "      <th>response (after)</th>\n",
       "      <th>rewards (before)</th>\n",
       "      <th>rewards (after)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;/s&gt;Sadly</td>\n",
       "      <td>I’m a male and this post\\nThis one is kind</td>\n",
       "      <td>, it's amazing.\\nIt is, and today is definitely a</td>\n",
       "      <td>0.958652</td>\n",
       "      <td>2.549457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;/s&gt;This movie is a</td>\n",
       "      <td>bore, people really need to chill.\\nI'd like ...</td>\n",
       "      <td>very unique my kindle game. It has hooked me ...</td>\n",
       "      <td>-1.982591</td>\n",
       "      <td>2.667962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;/s&gt;Quite average even by</td>\n",
       "      <td>Avalanche standards. He looked scary in three...</td>\n",
       "      <td>my standards.  I also love playing The Relevance</td>\n",
       "      <td>0.055134</td>\n",
       "      <td>2.123119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;/s&gt;In Le Million,</td>\n",
       "      <td>the sexual assault survivor gains only an att...</td>\n",
       "      <td>I really enjoyed giving people an opportunity...</td>\n",
       "      <td>-1.407230</td>\n",
       "      <td>2.139970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;/s&gt;Over several years of looking</td>\n",
       "      <td>forward to the season for the New</td>\n",
       "      <td>at both Iraq and Europhyphy</td>\n",
       "      <td>1.397924</td>\n",
       "      <td>0.839216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;/s&gt;Nothing new</td>\n",
       "      <td>… just an adjustment for me. It's always nice to</td>\n",
       "      <td>but I really enjoyed this episode. It was a r...</td>\n",
       "      <td>1.756747</td>\n",
       "      <td>2.705983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;/s&gt;So, what's</td>\n",
       "      <td>*repeatedly broken*?\\nthe servers are</td>\n",
       "      <td>your favorite thing I saw at the festival las...</td>\n",
       "      <td>-1.640984</td>\n",
       "      <td>1.032504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;/s&gt;Before watching this film</td>\n",
       "      <td>I'd let the guy die for Chris Hedges and Donn...</td>\n",
       "      <td>I was impressed with it.  Also, I'm pretty su...</td>\n",
       "      <td>-0.061161</td>\n",
       "      <td>2.376959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;/s&gt;Though not a horror film</td>\n",
       "      <td>based on the book, I</td>\n",
       "      <td>, Strong is a very interesting</td>\n",
       "      <td>0.429971</td>\n",
       "      <td>2.325984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;/s&gt;This is</td>\n",
       "      <td>life\\nNot really. If you have room</td>\n",
       "      <td>the most inspiring and emotionalweethed momen...</td>\n",
       "      <td>-0.167638</td>\n",
       "      <td>2.791420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;/s&gt;The worst, and ch</td>\n",
       "      <td>ock full of delusions, is Tyler's theory that ...</td>\n",
       "      <td>ases are easy. You have work\\nyeah i love it.\\n</td>\n",
       "      <td>-2.833605</td>\n",
       "      <td>1.849912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;/s&gt;There isn't a</td>\n",
       "      <td>place where blue is so spectacular, I love it...</td>\n",
       "      <td>isn't a 'playbook' but I love this.&lt;/s&gt;</td>\n",
       "      <td>2.573566</td>\n",
       "      <td>2.634705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;/s&gt;The concept was ok but</td>\n",
       "      <td>Zolpidem</td>\n",
       "      <td>I really enjoyed it</td>\n",
       "      <td>-1.450480</td>\n",
       "      <td>2.468080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;/s&gt;Some movies seem to be</td>\n",
       "      <td>underwater at the moment, and some are a litt...</td>\n",
       "      <td>really fun. I ended up nabbing one when I saw...</td>\n",
       "      <td>-0.369215</td>\n",
       "      <td>-1.195733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;/s&gt;I</td>\n",
       "      <td>can relate, finding the island gives me</td>\n",
       "      <td>'ve personally never seen mountains but that's</td>\n",
       "      <td>2.174362</td>\n",
       "      <td>0.599870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>&lt;/s&gt;I enjoyed watching Brigham Young and</td>\n",
       "      <td>his Roisting Bishop</td>\n",
       "      <td>his young students.</td>\n",
       "      <td>1.179537</td>\n",
       "      <td>1.862107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       query   \n",
       "0                                  </s>Sadly  \\\n",
       "1                        </s>This movie is a   \n",
       "2                  </s>Quite average even by   \n",
       "3                         </s>In Le Million,   \n",
       "4          </s>Over several years of looking   \n",
       "5                            </s>Nothing new   \n",
       "6                             </s>So, what's   \n",
       "7              </s>Before watching this film   \n",
       "8               </s>Though not a horror film   \n",
       "9                                </s>This is   \n",
       "10                     </s>The worst, and ch   \n",
       "11                         </s>There isn't a   \n",
       "12                </s>The concept was ok but   \n",
       "13                </s>Some movies seem to be   \n",
       "14                                     </s>I   \n",
       "15  </s>I enjoyed watching Brigham Young and   \n",
       "\n",
       "                                    response (before)   \n",
       "0          I’m a male and this post\\nThis one is kind  \\\n",
       "1    bore, people really need to chill.\\nI'd like ...   \n",
       "2    Avalanche standards. He looked scary in three...   \n",
       "3    the sexual assault survivor gains only an att...   \n",
       "4                   forward to the season for the New   \n",
       "5    … just an adjustment for me. It's always nice to   \n",
       "6               *repeatedly broken*?\\nthe servers are   \n",
       "7    I'd let the guy die for Chris Hedges and Donn...   \n",
       "8                                based on the book, I   \n",
       "9                  life\\nNot really. If you have room   \n",
       "10  ock full of delusions, is Tyler's theory that ...   \n",
       "11   place where blue is so spectacular, I love it...   \n",
       "12                                           Zolpidem   \n",
       "13   underwater at the moment, and some are a litt...   \n",
       "14            can relate, finding the island gives me   \n",
       "15                                his Roisting Bishop   \n",
       "\n",
       "                                     response (after)  rewards (before)   \n",
       "0   , it's amazing.\\nIt is, and today is definitely a          0.958652  \\\n",
       "1    very unique my kindle game. It has hooked me ...         -1.982591   \n",
       "2    my standards.  I also love playing The Relevance          0.055134   \n",
       "3    I really enjoyed giving people an opportunity...         -1.407230   \n",
       "4                         at both Iraq and Europhyphy          1.397924   \n",
       "5    but I really enjoyed this episode. It was a r...          1.756747   \n",
       "6    your favorite thing I saw at the festival las...         -1.640984   \n",
       "7    I was impressed with it.  Also, I'm pretty su...         -0.061161   \n",
       "8                      , Strong is a very interesting          0.429971   \n",
       "9    the most inspiring and emotionalweethed momen...         -0.167638   \n",
       "10    ases are easy. You have work\\nyeah i love it.\\n         -2.833605   \n",
       "11            isn't a 'playbook' but I love this.</s>          2.573566   \n",
       "12                                I really enjoyed it         -1.450480   \n",
       "13   really fun. I ended up nabbing one when I saw...         -0.369215   \n",
       "14     've personally never seen mountains but that's          2.174362   \n",
       "15                                his young students.          1.179537   \n",
       "\n",
       "    rewards (after)  \n",
       "0          2.549457  \n",
       "1          2.667962  \n",
       "2          2.123119  \n",
       "3          2.139970  \n",
       "4          0.839216  \n",
       "5          2.705983  \n",
       "6          1.032504  \n",
       "7          2.376959  \n",
       "8          2.325984  \n",
       "9          2.791420  \n",
       "10         1.849912  \n",
       "11         2.634705  \n",
       "12         2.468080  \n",
       "13        -1.195733  \n",
       "14         0.599870  \n",
       "15         1.862107  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### get a batch from the dataset\n",
    "bs = 16\n",
    "game_data = dict()\n",
    "dataset.set_format(\"pandas\")\n",
    "df_batch = dataset[:].sample(bs)\n",
    "game_data[\"query\"] = df_batch[\"query\"].tolist()\n",
    "query_tensors = df_batch[\"input_ids\"].tolist()\n",
    "\n",
    "response_tensors_ref, response_tensors = [], []\n",
    "\n",
    "#### get response from gpt2 and gpt2_ref\n",
    "for i in range(bs):\n",
    "    gen_len = output_length_sampler()\n",
    "    output = ref_model.generate(\n",
    "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    response_tensors_ref.append(output)\n",
    "    output = model.generate(\n",
    "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    response_tensors.append(output)\n",
    "\n",
    "#### decode responses\n",
    "game_data[\"response (before)\"] = [tokenizer.decode(response_tensors_ref[i]) for i in range(bs)]\n",
    "game_data[\"response (after)\"] = [tokenizer.decode(response_tensors[i]) for i in range(bs)]\n",
    "\n",
    "#### sentiment analysis of query/response pairs before/after\n",
    "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (before)\"])]\n",
    "game_data[\"rewards (before)\"] = [output[1][\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)]\n",
    "\n",
    "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (after)\"])]\n",
    "game_data[\"rewards (after)\"] = [output[1][\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7421c8",
   "metadata": {},
   "source": [
    "Видно, что модель теперь генерирует в основном только положительные тексты, даже если промт намекает на негативную тональность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810e15bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
