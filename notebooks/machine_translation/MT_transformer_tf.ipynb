{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0086f61",
   "metadata": {
    "id": "a0086f61"
   },
   "source": [
    "# Дисклеймер\n",
    "Эту тетрадку нужно запускать в колабе или в vast.ai. Не мучатесь с установкой библиотек и с обучением на cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b0ea3c",
   "metadata": {
    "id": "e3b0ea3c"
   },
   "outputs": [],
   "source": [
    "# !apt-get install unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d650e9eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d650e9eb",
    "outputId": "16aecd0e-9060-4289-f372-c1a2372d1953",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /Users/mnefedov/miniforge3/envs/tf2/lib/python3.9/site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in /Users/mnefedov/miniforge3/envs/tf2/lib/python3.9/site-packages (3.6.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/mnefedov/miniforge3/envs/tf2/lib/python3.9/site-packages (1.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/mnefedov/miniforge3/envs/tf2/lib/python3.9/site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/mnefedov/miniforge3/envs/tf2/lib/python3.9/site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mnefedov/miniforge3/envs/tf2/lib/python3.9/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/mnefedov/miniforge3/envs/tf2/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/mnefedov/miniforge3/envs/tf2/lib/python3.9/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/mnefedov/miniforge3/envs/tf2/lib/python3.9/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mnefedov/miniforge3/envs/tf2/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: numpy>=1.19 in /Users/mnefedov/miniforge3/envs/tf2/lib/python3.9/site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mnefedov/miniforge3/envs/tf2/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/mnefedov/miniforge3/envs/tf2/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/mnefedov/miniforge3/envs/tf2/lib/python3.9/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/mnefedov/miniforge3/envs/tf2/lib/python3.9/site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mnefedov/miniforge3/envs/tf2/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Error parsing requirements for pybind11: [Errno 2] No such file or directory: '/Users/mnefedov/miniforge3/envs/tf2/lib/python3.9/site-packages/pybind11-2.10.1.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tokenizers matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac85a206",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac85a206",
    "outputId": "3b604a02-892c-4828-9f61-ba31bb7aaa61"
   },
   "outputs": [],
   "source": [
    "# в vast ai или в последних версия jupyter может не работать автозаполнение, установка этой либы и перезагрука кернела помогает\n",
    "# %pip install --upgrade jedi==0.17.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96ff225",
   "metadata": {},
   "source": [
    "Большая часть этого семинара пересекается с видео от Andrey Karpathy - https://www.youtube.com/watch?v=kCc8FmEb1nY Возможно его объяснения покажутся вам лучше. \n",
    "Его видео более детальное и объемное, но фокус в нем скорее на GPT (декодере) для простой генерации текста, и еще оно на torch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8559a385",
   "metadata": {
    "id": "8559a385"
   },
   "source": [
    "# Транформеры для машинного перевода"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WjM0-Q75xtgk",
   "metadata": {
    "id": "WjM0-Q75xtgk"
   },
   "source": [
    "На huggingface очень много предобученных трансформеров, которые позволят вам решить очень большой процент рабочих/исследовательских задач. Однако бывают ситуации, когда нужной предобученной модели нет или она работает не очень хорошо. Обучить очень большую модель вряд ли получится (нужны видеокарты), но вот для средних моделей и специфичных задач может хватить даже колаба/каггла."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d2aa7",
   "metadata": {
    "id": "f19d2aa7"
   },
   "source": [
    "Давайте обучим свой небольшой трансформер на задаче машинного перевода. Это самая классическая seq2seq задача и подход, который мы размерем, применим и для других аналогичных задач. Для того, чтобы обучить модель решать какую-то другую seq2seq задачу вам нужно будет только подставить другие данные. Под seq2seq вообще при желании можно подвести любую задачу (классификация - это предсказание последовательности длинной 1 или можно предсказыть названия классов текстом). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "947b3313",
   "metadata": {
    "id": "947b3313"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import decoders\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508e8ae0",
   "metadata": {
    "id": "508e8ae0"
   },
   "source": [
    "Данные взяты вот отсюда - https://opus.nlpl.eu/opus-100.php (раздел с отдельными языковыми парами)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "kvUX7tdtc5pZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kvUX7tdtc5pZ",
    "outputId": "3a1609b9-4478-442e-9437-3d7b65544c84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-19 10:17:37--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
      "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
      "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 121340806 (116M)\n",
      "Saving to: ‘opus.en-ru-train.ru’\n",
      "\n",
      "opus.en-ru-train.ru 100%[===================>] 115,72M  36,9MB/s    in 3,1s    \n",
      "\n",
      "2023-03-19 10:17:40 (36,9 MB/s) - ‘opus.en-ru-train.ru’ saved [121340806/121340806]\n",
      "\n",
      "--2023-03-19 10:17:40--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
      "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
      "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 67760131 (65M)\n",
      "Saving to: ‘opus.en-ru-train.en’\n",
      "\n",
      "opus.en-ru-train.en 100%[===================>]  64,62M  35,3MB/s    in 1,8s    \n",
      "\n",
      "2023-03-19 10:17:42 (35,3 MB/s) - ‘opus.en-ru-train.en’ saved [67760131/67760131]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
    "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
    "# !wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
    "# !wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38911d06",
   "metadata": {
    "id": "38911d06"
   },
   "outputs": [],
   "source": [
    "# в русскоязычных данных есть \\xa0 вместо пробелов, он может некорректно обрабатываться токенизатором\n",
    "text = open('opus.en-ru-train.ru').read().replace('\\xa0', ' ')\n",
    "f = open('opus.en-ru-train.ru', 'w')\n",
    "f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e110ff04",
   "metadata": {
    "id": "e110ff04"
   },
   "outputs": [],
   "source": [
    "en_sents = open('opus.en-ru-train.en').read().lower().splitlines()\n",
    "ru_sents = open('opus.en-ru-train.ru').read().lower().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b8fae3",
   "metadata": {
    "id": "35b8fae3"
   },
   "source": [
    "Пример перевода с английского на русский"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eb9b498",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0eb9b498",
    "outputId": "277adddf-1f07-4960-9b0f-b8377f6add0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('so what are you thinking?', 'ну и что ты думаешь?')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sents[-1], ru_sents[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bbf529",
   "metadata": {
    "id": "31bbf529"
   },
   "source": [
    "Как обычно нам нужен токенизатор, а точнее даже 2, т.к. у нас два корпуса. Будем использовать wordpiece токенайзер из библиотеки tokenizers от huggingface. Wordpiece разбивает текст на токены, которые могут быть как целыми словами, так и кусками слов и даже отдельными символами. \n",
    "В токенизатор мы передаем еще несколько специальных символов - UNK (это символ замена для неизвестных слов), START - этот токен будет добавлен в начало текста, END - этот символ будет добавлен в конце, PAD - это индекс паддинга\n",
    "\n",
    "В токенезаторе для целевого языка по идее нам не нужен UNK, потому что в переводном тексте технических токенов быть не должно, но мы не можем без него обойтись, потому что мы ограничиваем размер словаря. Можно было бы построить словарь так, чтобы в нем были все возможные токены и тогда UNK был бы не нужен, но тогда размер словаря будет слишком большим. Но мы всегда можем отфильтровать UNK из перевода, просто не разрешая модели предсказывать UNK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0be060a4",
   "metadata": {
    "id": "0be060a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer_en = Tokenizer(WordPiece(), )\n",
    "tokenizer_en.normalizer = normalizers.Sequence([Lowercase()])\n",
    "tokenizer_en.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer_en = WordPieceTrainer(\n",
    "          vocab_size=30000, special_tokens=[\"[UNK]\", \"[PAD]\"])\n",
    "tokenizer_en.train(files=[\"opus.en-ru-train.en\"], trainer=trainer_en )\n",
    "\n",
    "tokenizer_ru = Tokenizer(WordPiece(), )\n",
    "tokenizer_ru.normalizer = normalizers.Sequence([Lowercase()])\n",
    "tokenizer_ru.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer_ru = WordPieceTrainer(\n",
    "          vocab_size=30000, special_tokens=[\"[UNK]\", \"[PAD]\", \"[START]\", \"[END]\", ])\n",
    "tokenizer_ru.train(files=[\"opus.en-ru-train.ru\"], trainer=trainer_ru )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "xWX3xnMUzbdt",
   "metadata": {
    "id": "xWX3xnMUzbdt"
   },
   "outputs": [],
   "source": [
    "tokenizer_en.decoder = decoders.WordPiece()\n",
    "tokenizer_ru.decoder = decoders.WordPiece()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FZLOU_Qe4Tw-",
   "metadata": {
    "id": "FZLOU_Qe4Tw-"
   },
   "source": [
    "Когда мы будет раскодировать предсказанные индексы в слова нам пригодится декодер. Лучше использовать готовый из той же библиотеки. Он сам склеит символьные нграммы в слова. Без него нам нужно было бы писать правила, чтобы избавиться от вот такого"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "EVuGuhHg3-Tq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "EVuGuhHg3-Tq",
    "outputId": "4e66aeac-a7b6-46d0-a849-23e1667e13ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'пример текста с ред ##ким словом'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# без декодера\n",
    "' '.join(tokenizer_ru.encode('Пример текста с редким словом').tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f01e7bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'пример текста с редким словом'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c готовым декодером\n",
    "tokenizer_ru.decoder.decode(tokenizer_ru.encode('Пример текста с редким словом').tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605ea30c",
   "metadata": {
    "id": "605ea30c"
   },
   "source": [
    "### ВАЖНО!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f07127",
   "metadata": {
    "id": "67f07127"
   },
   "source": [
    "Токенизатор - это неотъемлимая часть модели, поэтому не забывайте сохранять токенизатор вместе с моделью. Если вы забудете про это и переобучите токенизатор, то индексы токенов разойдутся и веса модели будут бесполезны. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "496d0ea7",
   "metadata": {
    "id": "496d0ea7"
   },
   "outputs": [],
   "source": [
    "# раскоментируйте эту ячейку при обучении токенизатора\n",
    "# а потом снова закоментируйте чтобы при перезапуске не перезаписать токенизаторы\n",
    "tokenizer_en.save('tokenizer_en')\n",
    "tokenizer_ru.save('tokenizer_ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4661964",
   "metadata": {
    "id": "f4661964"
   },
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer.from_file(\"tokenizer_en\")\n",
    "tokenizer_ru = Tokenizer.from_file(\"tokenizer_ru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0685f9",
   "metadata": {
    "id": "2b0685f9"
   },
   "source": [
    "##### Переводим текст в индексы. \n",
    "\n",
    "Для исходного языка нам не нужно никаких дополнительных токенов, поэтому просто вызываем токенизатор.   \n",
    "А вот для целевого в начало нужно добавить токен '[START]', а в конец '[END]'.  \n",
    "Эти теги понадобятся, когда мы будем генерировать тексты с уже обученной моделью - тэг [START] мы будем подавать в самом начале перевода, чтобы состояние декодера не было пустым (ниже вы увидите, почему оно не может быть пустым), а [END] будем использовать, чтобы понимать когда остановить перевод (если модель предсказала [END] - значит перевод окончен)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc003758",
   "metadata": {
    "id": "dc003758"
   },
   "outputs": [],
   "source": [
    "def encode(text, tokenizer, target=False):\n",
    "    if target:\n",
    "        return [tokenizer.token_to_id('[START]')] + tokenizer.encode(text).ids + \\\n",
    "                [tokenizer.token_to_id('[END]')]\n",
    "    else:\n",
    "        return tokenizer.encode(text).ids \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff04bf47",
   "metadata": {
    "id": "ff04bf47"
   },
   "source": [
    "Кодируем и паддим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fc2dae1",
   "metadata": {
    "id": "7fc2dae1"
   },
   "outputs": [],
   "source": [
    "X_en = [encode(t, tokenizer_en) for t in en_sents]\n",
    "X_ru = [encode(t, tokenizer_ru, True) for t in ru_sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa68238e",
   "metadata": {},
   "source": [
    "Как обычно мы должны выбрать максимальную допустимую длину. Максимальные значения слишком большие"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "281b5b90",
   "metadata": {
    "id": "281b5b90"
   },
   "outputs": [],
   "source": [
    "max_len_en = np.max([len(x) for x in X_en])\n",
    "max_len_ru = np.max([len(x) for x in X_ru])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5984886a",
   "metadata": {
    "id": "5984886a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17863, 19389)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_en, max_len_ru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad8e2ae",
   "metadata": {},
   "source": [
    "Если посмотреть на перцентили, то будет видно, что это скорее всего выбросы. Большая часть текстов короткие."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49f2f2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный язык\n",
      "50 % текстов <=  10.0\n",
      "75 % текстов <=  18.0\n",
      "90 % текстов <=  33.0\n",
      "95 % текстов <=  45.0\n",
      "99 % текстов <=  75.0\n"
     ]
    }
   ],
   "source": [
    "print('Исходный язык')\n",
    "print('50 % текстов <= ', np.percentile([len(x) for x in X_en], 50))\n",
    "print('75 % текстов <= ', np.percentile([len(x) for x in X_en], 75))\n",
    "print('90 % текстов <= ', np.percentile([len(x) for x in X_en], 90))\n",
    "print('95 % текстов <= ', np.percentile([len(x) for x in X_en], 95))\n",
    "print('99 % текстов <= ', np.percentile([len(x) for x in X_en], 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ab5bcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Целевой язык\n",
      "50 % текстов <=  11.0\n",
      "75 % текстов <=  20.0\n",
      "90 % текстов <=  36.0\n",
      "95 % текстов <=  48.0\n",
      "99 % текстов <=  79.0\n"
     ]
    }
   ],
   "source": [
    "print('Целевой язык')\n",
    "print('50 % текстов <= ', np.percentile([len(x) for x in X_ru], 50))\n",
    "print('75 % текстов <= ', np.percentile([len(x) for x in X_ru], 75))\n",
    "print('90 % текстов <= ', np.percentile([len(x) for x in X_ru], 90))\n",
    "print('95 % текстов <= ', np.percentile([len(x) for x in X_ru], 95))\n",
    "print('99 % текстов <= ', np.percentile([len(x) for x in X_ru], 99))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab64c02",
   "metadata": {},
   "source": [
    "Давайте для каждого из языков возьмем значения, которые покрывают 95 % всех данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5cc0a376",
   "metadata": {
    "id": "5cc0a376"
   },
   "outputs": [],
   "source": [
    "# обратите внимание, что в seq2seq длины могут быть разными\n",
    "max_len_en, max_len_ru = 45, 48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc8070",
   "metadata": {},
   "source": [
    "По умолчанию паддинг делается нулями, но нам это не подходит, так как в постороенных словарях 0 - это тэг UNK. Вытащим тэг паддинга из словаря и будет использовать его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f4f31fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f4f31fa",
    "outputId": "85345521-b21a-4bc1-96e2-028ecd737034"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD_IDX = tokenizer_ru.token_to_id('[PAD]')\n",
    "PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "49ae735e",
   "metadata": {
    "id": "49ae735e"
   },
   "outputs": [],
   "source": [
    "X_en = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "              X_en, maxlen=max_len_en, padding='post', value=PAD_IDX)\n",
    "\n",
    "X_ru_out = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "              [x[1:] for x in X_ru], maxlen=max_len_ru-1, padding='post', \n",
    "              value=PAD_IDX)\n",
    "\n",
    "X_ru_dec = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "              [x[:-1] for x in X_ru], maxlen=max_len_ru-1, \n",
    "              padding='post', value=PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae14749",
   "metadata": {},
   "source": [
    "На вход модели мы будем подавать три матрицы.\n",
    "\n",
    "Первая - X_en - это индексы токенов в английских текстах (тут все как и в предыдущих семинарах)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62d4e3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"yeah, that's not exactly...\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "597680b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3280,   13, 2763,    8,   58, 2808, 5148, 2856,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_en[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3f4c4e",
   "metadata": {},
   "source": [
    "А вот русскоязычные тексты мы будем подавать через две матрицы - одна будет использоваться как вход для декодера (X_ru_dec), а вторая как правильные ответы (X_ru_out). \n",
    "\n",
    "Если присмотреться, то можно заметить что X_ru_dec и X_ru_out отличаются на 1 шаг (X_ru_out это X_ru_dec сдвинутый вправо)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d751b1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'да, но не совсем...'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed95081e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2, 2588,   15, 2589, 2513, 5362, 2643,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ru_dec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14d7c918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2588,   15, 2589, 2513, 5362, 2643,    3,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ru_out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79452bf4",
   "metadata": {},
   "source": [
    "Такая структура внутри модели будет использована, чтобы предсказывать следующее слово по текущему контексту. Грубо говоря, модель будет интерпретировать эти две последовательности индексов, как пары контекст-продолжение. \n",
    "\n",
    "Вот это:  \n",
    "2, 2588, 15, 2589, 2513, 5362, 2643   \n",
    "2588, 15, 2589, 2513, 5362, 2643,    3\n",
    "\n",
    "Превратится для модели в:  \n",
    "2 - 2588  \n",
    "2, 2588 - 15  \n",
    "2, 2588, 15, 2589 - 2513  \n",
    "2, 2588, 15, 2589, 2513 - 5362  \n",
    "2, 2588, 15, 2589, 2513, 5362 - 2643  \n",
    "2, 2588, 15, 2589, 2513, 5362, 2643 - 3  \n",
    "\n",
    "(как это будет сделано показано дальше в разделе про маски)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "824dcb29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "824dcb29",
    "outputId": "34899386-a168-4cd3-b063-71e305cc1c22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000000, 45), (1000000, 47))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# миллион примеров \n",
    "X_en.shape, X_ru_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfa4331",
   "metadata": {
    "id": "acfa4331"
   },
   "source": [
    "Разделяем на трейн и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "25fa5f05",
   "metadata": {
    "id": "25fa5f05"
   },
   "outputs": [],
   "source": [
    "(X_en_train, X_en_valid, \n",
    "X_ru_dec_train, X_ru_dec_valid, \n",
    "X_ru_out_train, X_ru_out_valid) = train_test_split(X_en, \n",
    "                                                  X_ru_dec, \n",
    "                                                  X_ru_out, \n",
    "                                                  test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9faa719",
   "metadata": {
    "id": "d9faa719"
   },
   "source": [
    "Дальше код модели, он взят вот отсюда (с небольшими изменениями) - https://www.tensorflow.org/text/tutorials/transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f045ac37",
   "metadata": {
    "id": "f045ac37"
   },
   "source": [
    "## Имплементация трансформера"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4270b2f",
   "metadata": {},
   "source": [
    "Главное в трансформере это механизм внимания. Функция scaled_dot_product_attention преобразует query, key, value вектора в обновленные вектора с помощью механизма внимания.\n",
    "\n",
    "Визуализация этих шагов (источник: https://jalammar.github.io/illustrated-transformer/):\n",
    "![](https://jalammar.github.io/images/t/self-attention-output.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e70b1a",
   "metadata": {},
   "source": [
    "Пройдитесь по каждой строчке в этой функции и сопоставьте с визуализацией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "656be820",
   "metadata": {
    "id": "656be820"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    # Считаем скалярное произведение между запросом (query) и ключом (key), транспонируя ключ\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # Получаем глубину (размерность) ключа и преобразуем ее во float\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "\n",
    "    # Делим результат скалярного произведения на квадратный корень из глубины\n",
    "    # Это делается для уменьшения влияния больших значений и стабилизации градиентов во время обучения\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # Если есть маска, применяем ее к логитам, чтобы обнулить нежелательные значения\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # Применяем функцию softmax для получения весов внимания\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    # Умножаем веса внимания на значения (value) для получения итогового результата\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efb7349",
   "metadata": {},
   "source": [
    "В этой функции query, key и value вектора подаются на вход, но как они получаются из эмбедингов?\n",
    "\n",
    "Это реализовано в классе MultiHeadAttention, который уже является самостоятельным слоем, на котором потом будет построен блок трансформера как в энкодере так и в декодере.\n",
    "\n",
    "В этом слое можно увидеть три отдельных полносвязных слоя одинаковой размерности (d_model), которые применяются к inputs, чтобы получить q, k, v вектора.  \n",
    "Можно заметить, что в функции call, разделение на q,k,v происходит еще раньше - они просто достаются из inputs по ключу, как из словаря. Но если немного промотать вперед до encoder_layer , то можно убедиться, что при вызове MultiheadAttention в эти переменные подставляются одно и то же (нажмите CTRL+F и введите #call_mha, чтобы перейти к этому моменту и потом вернутся сюда). \n",
    "\n",
    "А если сделать еще один шаг до encoder, то можно увидеть, что такое inputs - это эмбединги слов (CTRL+F и введите #inputs). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f4b51870",
   "metadata": {
    "id": "f4b51870"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads  # количество голов для внимания\n",
    "        self.d_model = d_model  # размерность вектора модели\n",
    "\n",
    "        # Убеждаемся, что размерность модели делится нацело на количество голов\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads  # размерность каждой головы\n",
    "\n",
    "        # Создаем полносвязные слои для запроса, ключа и значения\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        # Создаем последний полносвязный слой\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        # Разделяем входные данные на головы\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "            'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # Пропускаем запрос, ключ и значение через соответствующие полносвязные слои\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # Разделяем запрос, ключ и значение на головы \n",
    "        # то есть просто разрезаем вектора на num_heads частей \n",
    "        # и сравниваем все части между собой\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # Выполняем механизм внимания с масштабированным скалярным произведением\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # Объединяем головы вместе (склеиваем векторы в один)\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))\n",
    "\n",
    "        # Пропускаем объединенное внимание через дополнительный полносвязный слой\n",
    "        # Он просто добавляет сложности нашей модели\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f10e40",
   "metadata": {},
   "source": [
    "#### Теперь наверное самый сложный момент - почему это MultiHead, а не просто Attention layer? \n",
    "\n",
    "В MultiheadAttention используются не по одному q,k,v вектору, а сразу по N векторов, но меньшего размера. В классе выше N задается через парметр num_heads и размерность отдельного q,k или v вектора будет равна d_model/num_heads. \n",
    "\n",
    "Внимание рассчитывается между соответствующими парами q и v внутри каждой головы, а итоговый вектор получается объединением результата внимания от всех голов (его размерность будет равна d_model).  \n",
    "\n",
    "В статье illustrated transformer этот момент визуализируется вот так:\n",
    "![](https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png)\n",
    "\n",
    "Но реализация в TF немного отличается. На этой картинке на шаге 3 показано, что полносвязные слои, через которые получаются q,k,v векторы у каждой головы свои - и их количество не 3, а 3 умножить на количество голов. А вот в TF полносвязных слоя всего 3, но дальше вектора просто разрезаются на куски и начаная с шага 4, все уже совпадает. \n",
    "\n",
    "Если задуматься, то разницы между этими подходами нет. Полносвязный слой это матрица, на которую умножается входной вектор, в результате чего получается другой вектор. И каждое значение в новом векторе - это результат попарного умножения и суммы значений в исходном векторе и отдельной колонки в матрице. Таким образом, уможить на матрицу и разрезать результат на три куска - это то же самое, что разрезать матрицу на три куска и умножить вектор на каждый из кусков отдельно:\n",
    "\n",
    "![](https://i.ibb.co/9Yg580J/image.png)\n",
    "\n",
    "\n",
    "Давайте еще отдельно посмотрим как происходит разбиение на головы и как потом выглядит скалярное произведение для отдельного куска."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5760c73c",
   "metadata": {},
   "source": [
    "Инициализируем класс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "19dd941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(d_model=10, num_heads=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd8fa2d",
   "metadata": {},
   "source": [
    "Сгенерируем батч из одного текста с двумя словами и размером эбмединга 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7c25cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = np.random.normal(size=(1, 2, 3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2fa2e4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.35202364, -0.25058994,  1.56498003],\n",
       "        [-1.38529637, -0.49439637, -2.32138754]]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14f09c0",
   "metadata": {},
   "source": [
    "Сформируем входной словарь для класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7e866e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {'query': inp,'key': inp, 'value': inp, 'mask':None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819a35c3",
   "metadata": {},
   "source": [
    "Применим слой к входному словарю"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d0b7823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = mha(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61747204",
   "metadata": {},
   "source": [
    "На выходе получилось, что для каждого слова у нас теперь эмдединг размером 10 (мы указали такой d_model выше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8e5bb366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 2, 10])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b016f9",
   "metadata": {},
   "source": [
    "Давайте посмотрим как это вектор вычислялся внутри класса по шагам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0cdee43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# преобразуем эмбединг в q,k,v вектора\n",
    "q = mha.query_dense(inputs['query'])\n",
    "k = mha.query_dense(inputs['key'])\n",
    "v = mha.query_dense(inputs['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "33465f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 2, 10])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# размерность не изменилась так как мы еще не порезали на головы\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1a818c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# разрезаем каждый вектор на куски (головы)\n",
    "qh = mha.split_heads(q, tf.shape(q)[0])\n",
    "kh = mha.split_heads(k, tf.shape(k)[0])\n",
    "vh = mha.split_heads(v, tf.shape(v)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0a7f8b",
   "metadata": {},
   "source": [
    "В результате получается тензор с размерностями (количество текстов x количество слов x количество голов x размерность вектора головы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1c182c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 2, 2, 5])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# мы указали num_heads 2 - поэтому размерность каждого куска 5 (10 разделить на два)\n",
    "qh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6cbb9578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([5, 2, 2, 1])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(kh).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07c7527",
   "metadata": {},
   "source": [
    "Далее вычисляется скалярное произведение между q и k кусками. На вход подаются тензоры, но тензорфлоу умеет работать с таким форматом тоже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a7f9d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matmul_qk = tf.matmul(qh, kh, transpose_b=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5a2ef8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 2, 2, 2])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# На выходе получится тензор вот такой размерности\n",
    "# 5 изменилось на 2\n",
    "# так как для каждого куска у нас по две близости (до каждого слова включая себя)\n",
    "matmul_qk.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e837c492",
   "metadata": {},
   "source": [
    "Применим софтмакс, чтобы нагляднее увидеть близости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "85a6d549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.89, 0.11],\n",
       "         [0.06, 0.94]],\n",
       "\n",
       "        [[0.96, 0.04],\n",
       "         [0.  , 1.  ]]]], dtype=float32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(matmul_qk, axis=-1).numpy().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c01fb05",
   "metadata": {},
   "source": [
    "Далее эти близости используются как веса для суммы value кусков и потом результат по каждому кусочку собирается обратно в один вектор размерности (1, 2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e638a",
   "metadata": {},
   "source": [
    "### Маски\n",
    "\n",
    "Еще один важный момент это то как используются маски, чтобы занулить некоторые близости.\n",
    "Есть два вида масок - маска для паддинга и маска для слов в будущем. \n",
    "Маска для паддинга используется и в энкодере и в декодере, а маска для слов в будущем только для декодера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5c48cea2",
   "metadata": {
    "id": "5c48cea2"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, PAD_IDX), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "64790293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 1, 3), dtype=float32, numpy=array([[[[1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_padding_mask([[1,2,3]]) # 1 подставилась для индекса который равен PAD_IDX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0ab9ad",
   "metadata": {},
   "source": [
    "Маска для будущего создает треугольную матрицу, которая накладывается на близости в self-attention части декодера. Как раз с помощью этой маски и реализуется разбиение входных последовательностей на контекст-продолжение - сначала вычисляется внимание до всех слов, даже в будущем, но потом полученные близости слов в будущем просто зануляются и не используются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "21c0c899",
   "metadata": {
    "id": "21c0c899"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b6f988e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# для декодера нам нужно замаскировать следующие токены\n",
    "# так как мы пытаемся их сгенерировать\n",
    "# для этого создается вот такая маска\n",
    "mask = 1 - tf.linalg.band_part(tf.ones((3, 3)), -1, 0)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "499d2f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[  -0., -100., -100.],\n",
       "       [  -0.,   -0., -100.],\n",
       "       [  -0.,   -0.,   -0.]], dtype=float32)>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# в слое с вниманием эта маска будет использоваться чтобы занулить близости с токенами из будущего\n",
    "(mask * -100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9c0f62e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=\n",
       "array([[[[1., 1., 1.],\n",
       "         [1., 0., 1.],\n",
       "         [1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# эта функция также проверяет и паддинг и если он есть то и его тоже замаскирует\n",
    "create_look_ahead_mask([[1,3,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7a1f209e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=\n",
       "array([[[[0., 1., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_look_ahead_mask([[2,4,3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c019796a",
   "metadata": {},
   "source": [
    "### Позиционное кодирование\n",
    "\n",
    "К каждому эбмедингу слова добавляется вектор, который зависит от позиции слова. Таким образом в трансформер передается информация о порядке слов. Обратите внимание, что выше вектора сравниваются через скалярное произведение просто попарно и все вектора, которые не замаскированы, можно переставить местами и результат не изменится. Добавление информации о позиции в сами вектора изменяет результат.\n",
    "\n",
    "Класс PositionalEncoding генерирует вектор для каждой позиции заданной размерности. Размерность задается также через d_model, чтобы можно было просто прибавить к вектору слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e120bbe5",
   "metadata": {
    "id": "e120bbe5"
   },
   "outputs": [],
   "source": [
    "## в туториале от tensorflow принцип работы postional encoding описан подробнее\n",
    "## https://www.tensorflow.org/text/tutorials/transformer\n",
    "## но для семинара это не так важно\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66fcc12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c7fb39e",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7763b910",
   "metadata": {},
   "source": [
    "Дальше кастомные функции и классы, которые были определены выше и стандартные инструменты tf.keras объединяются, чтобы собрать эндкодер и декодер. Энкодер устроен вот так (источник - https://jalammar.github.io/illustrated-transformer/)\n",
    "![](https://jalammar.github.io/images/t/transformer_resideual_layer_norm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3039bada",
   "metadata": {},
   "source": [
    "Выше мы разобрались с self-attention и positional encoding. Теперь к этому нужно добавить еще один полносвязный слой (а точнее даже два), а также нормализацию и skip связи (пунктирные стрелочки). Ну и Dropout слои, но это просто случайное зануление некоторых векторов, чтобы модель лучше обучалась, а не что-то существенное, поэтому их даже нет на картинке.\n",
    "\n",
    "Нормализация делается с помощью LayerNormalization слоя. Она нужна, чтобы значения получаемых векторов оставались в каких-то разумных пределах. А skip-связи реализованы просто через суммирование - обратите внимание, как переменная inputs сначала передается в MultiHeadAttention, а потом полученный результат прибавляется к исходной inputs; и далее attention (который является суммой inputs и MHA(inputs)) передается в два полносвязных слоя и полученный результат прибавляется к изначальной attention переменной. В итоге получается, что информация как бы постепенно добавляется к изначальным векторам, а не перезаписывается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "68472627",
   "metadata": {
    "id": "68472627"
   },
   "outputs": [],
   "source": [
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    #call_mha\n",
    "    attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c5f72e",
   "metadata": {},
   "source": [
    "Блоков энкодера может быть сколько угодно, так как один такой блок принимает на вход векторы размерности d_model и выдает как результат вектора размерности d_model, т.е. можно скармливать результаты одного блока другому и так далее. За количество последовательных транформерных блоков в энкодере отвечает параметр num_layers. Обратите внимание, что ниже все это реализовано просто через цикл с перезаписыванием переменной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "89dcc42e",
   "metadata": {
    "id": "89dcc42e"
   },
   "outputs": [],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            max_len,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
    "\n",
    "    #inputs (они тут называются outputs но это просто такой нейминг, \n",
    "    # этот параметр передается в encoder_layer первым и encoder_layer будет считать его inputs)\n",
    "    # outputs он тут называется для удобства, так как он будет перезаписываться\n",
    "    # чтобы на вход следующему блоку подавать уже не эмбединги,\n",
    "    # а то что получится как результат предыдущего блока\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d048be7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2380074",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Декодер построен из точно таких же частей с одним важным отличием - в одном блоке декодера два MultiHeadAttention слоя. Первый из них такой же как в энкодере и реализует self-attention, а во втором реализован cross-attention. \n",
    "\n",
    "![](https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809b26df",
   "metadata": {},
   "source": [
    "В коде разница выражает в параметрах, которые передаются в MultiHeadAttention. Обратите внимание, что ниже первый MultiHeadAttention вызывается также как и в энкодере (inputs={'query': inputs, 'key': inputs, 'value': inputs, 'mask': look_ahead_mask}), а вот на вход второму передаются inputs={ 'query': attention1, 'key': enc_outputs, 'value': enc_outputs, 'mask': padding_mask}, то есть query часть тут это inputs декодера уже обогащенный self-attention, а key и value это последние значения этих векторов в энкодере. Еще важно заметить, что в первом MultiHeadAttention используется look_ahead_mask, а во втором только padding_mask.\n",
    "\n",
    "Таким образом, в декодере обращается внимание на уже сгенерированный перевод и на весь исходный текст. \n",
    "\n",
    "Обратите внимание, что вход декодера не может быть пустым, потому что тогда inputs будет пустым и не будет query, key, value векторов между которыми можно расчитать внимание и соответственно не из чего делать предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a2f90e7c",
   "metadata": {
    "id": "a2f90e7c"
   },
   "outputs": [],
   "source": [
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1c0dd6a2",
   "metadata": {
    "id": "8e9f89b8"
   },
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            max_len,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd2160e",
   "metadata": {},
   "source": [
    "### Модель целиком\n",
    "\n",
    "Общая модель состоит из энкодера, декодера и еще 1 полносвязного слоя. Дополнительный последний слой нужен, чтобы из векторов получить вероятности классов (=слов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e356741b",
   "metadata": {
    "id": "e356741b"
   },
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                max_len,\n",
    "                name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "    \n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "    \n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "    enc_outputs = encoder(\n",
    "                          vocab_size=vocab_size[0],\n",
    "                          num_layers=num_layers,\n",
    "                          units=units,\n",
    "                          d_model=d_model,\n",
    "                          num_heads=num_heads,\n",
    "                          dropout=dropout,\n",
    "                          max_len=max_len[0],\n",
    "                        )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "    dec_outputs = decoder(\n",
    "                          vocab_size=vocab_size[1],\n",
    "                          num_layers=num_layers,\n",
    "                          units=units,\n",
    "                          d_model=d_model,\n",
    "                          num_heads=num_heads,\n",
    "                          dropout=dropout,\n",
    "                          max_len=max_len[1],\n",
    "                        )(inputs=[dec_inputs, enc_outputs, \n",
    "                                  look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size[1], name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adc39ee",
   "metadata": {},
   "source": [
    "Функция потерь тут реализована отдельно, чтобы не учитывать паддинги в расчетах. Поэтому выше последний полносвязный слой был без софтмакса - если применить софтмакс, а потом какие-то вероятности занулить, то распределение сломается (не будет суммироваться в 1). Поэтому указан параметр from_logits=True, который как раз означает, что на вход подаются не вероятности, а логиты, то есть просто результат полносвязного слоя без активации.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d23ece2d",
   "metadata": {
    "id": "6c35ce0f"
   },
   "outputs": [],
   "source": [
    "\n",
    "L  = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none',)\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    loss = L(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, PAD_IDX), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4743513",
   "metadata": {
    "id": "a4743513"
   },
   "source": [
    "Определяем параметры модели. Есть стандартные наборы параметров для обучения маленькой, средней или большой модели. Их можно посмотреть в оригинальной статье про трансформер - https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "Мы возьмем маленький, т.к. он требует меньше ресурсов. НО для практической задачи есть смысл использовать модель побольше, есть статьи в которые показано, что большие трансформеры требуют даже меньше данных для обучения -  https://arxiv.org/pdf/2002.11794.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "542fcc43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "542fcc43",
    "outputId": "c9359c72-3dfb-4985-b26b-a832fc6fb080"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# small model\n",
    "NUM_LAYERS = 2\n",
    "D_MODEL = 256\n",
    "NUM_HEADS = 8\n",
    "\n",
    "UNITS = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "\n",
    "# average model\n",
    "# NUM_LAYERS = 6\n",
    "# D_MODEL = 512\n",
    "# NUM_HEADS = 8\n",
    "# UNITS = 2048\n",
    "# DROPOUT = 0.1\n",
    "\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=(tokenizer_en.get_vocab_size(),tokenizer_ru.get_vocab_size()),\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    max_len=[max_len_en, max_len_ru])\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    0.001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('model_ruen',\n",
    "                                            monitor='val_loss',\n",
    "                                            verbose=1,\n",
    "                                            save_weights_only=True,\n",
    "                                            save_best_only=True,\n",
    "                                            mode='min',\n",
    "                                            save_freq='epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6590325",
   "metadata": {
    "id": "c6590325"
   },
   "source": [
    "Трансформеры - большие модели, обучаться они должны долго. Размер батча здесь уже имеет большое значение: сликом маленький батч приведет к сильному увеличению времени на обучение, но сильно большим его поставить не получится, т.к. модель большая и быстро займет всю видеокарту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a8e81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "308a8e81",
    "outputId": "981be9b3-524f-4e0a-fc93-c44bddd4bf3c"
   },
   "outputs": [],
   "source": [
    "model.fit((X_en_train, X_ru_dec_train), X_ru_out_train, \n",
    "             validation_data=((X_en_valid, X_ru_dec_valid), X_ru_out_valid),\n",
    "             batch_size=500,\n",
    "             epochs=1,\n",
    "             callbacks=[checkpoint]\n",
    "             )\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61cb6ed",
   "metadata": {
    "id": "d61cb6ed"
   },
   "source": [
    "Можно попробовать перевести рандомное предложение и оценить качество модели самому"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "63762869",
   "metadata": {
    "id": "63762869"
   },
   "outputs": [],
   "source": [
    "def translate(text):\n",
    "    input_ids = encode(text, tokenizer_en, target=False)\n",
    "\n",
    "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                                      [input_ids], maxlen=max_len_en, padding='post', \n",
    "                                       # важно не забыть паддинг с нужным id\n",
    "                                       value=PAD_IDX)\n",
    "\n",
    "    \n",
    "    \n",
    "    output_ids = [tokenizer_ru.token_to_id('[START]') ]\n",
    "    \n",
    "    pred = model((input_ids, tf.cast([output_ids], tf.int32)), training=False).numpy()\n",
    " \n",
    "    \n",
    "    while pred.argmax(2)[0][-1] not in [tokenizer_ru.token_to_id('[END]')]:\n",
    "        if len(output_ids) >= max_len_ru:\n",
    "            break\n",
    "        # можно занизить скор тэга UNK чтобы он никогда не генерировался\n",
    "        pred[:, :, tokenizer_ru.token_to_id('[UNK]')] = -100\n",
    "\n",
    "        output_ids.append(pred.argmax(2)[0][-1])\n",
    "        pred = model((input_ids, tf.cast([output_ids], tf.int32)), training=False).numpy()\n",
    "\n",
    "    return tokenizer_ru.decode(output_ids[1:], )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5f16e8b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "5f16e8b5",
    "outputId": "b974fb32-2c36-44bc-8de9-df7007515f58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'трансформатор'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7d0ee1b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "7d0ee1b8",
    "outputId": "9fd418f9-1636-4b89-f931-599086753df9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'вы можете перевести это предложение?'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"can you translate this sentence?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "aa0b57d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "aa0b57d2",
    "outputId": "cdeea84c-8dae-42f4-c63b-b09b2938c697"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'пожалуйста перевести это предложение в русский'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"please translate this sentence into russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fc8dbbbd",
   "metadata": {
    "id": "fc8dbbbd"
   },
   "outputs": [],
   "source": [
    "# translate(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a5e2071b",
   "metadata": {
    "id": "a5e2071b"
   },
   "outputs": [],
   "source": [
    "# почему без старт и енд в енкодере хуже???\n",
    "# ссылки на доп материалы\n",
    "# как примменяется паддинг маск к атеншену"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-ZPgGsEl8Cul",
   "metadata": {
    "id": "-ZPgGsEl8Cul"
   },
   "source": [
    "# BLEU\n",
    "\n",
    "Оценивание машинного перевода очень сложная задача. Главная сложность в том, что хороших переводов одного текста может быть несколько, а есть обычно только 1. Технического решения этой проблемы пока не придумали и если нужна достоверная оценка, то используют людей или даже профессиональных переводчиков. Для автоматической оценки стандартно используется простая метрика BLEU. Она считает пересечения между двумя переводами на уровне отдельных токенов, биграммов, триграммов и четырехграммов. Чем больше полных пересечений, тем лучше. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MX9WvnhW_UNV",
   "metadata": {
    "id": "MX9WvnhW_UNV"
   },
   "source": [
    "Реализация bleu есть в nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2735e9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 4.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
      "Collecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 4.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
      "\u001b[K     |████████████████████████████████| 772 kB 37.0 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm, click, regex, nltk\n",
      "Successfully installed click-8.1.3 nltk-3.8.1 regex-2022.10.31 tqdm-4.65.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "YbuJQX9S8E0o",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YbuJQX9S8E0o",
    "outputId": "89d78b1a-8c15-44c3-c54b-7a2fc45dcbea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4548019047027907\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "hypothesis = ['It', 'is', 'a', 'cat', 'at', 'room']\n",
    "reference = ['It', 'is', 'a', 'cat', 'inside', 'the', 'room']\n",
    "\n",
    "\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, auto_reweigh=True)\n",
    "print(BLEUscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rdv_0kXKH94I",
   "metadata": {
    "id": "Rdv_0kXKH94I"
   },
   "source": [
    "Давайте расчитаем BLEU на 10 примерах из тестового корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "TbLvCxxlFCn6",
   "metadata": {
    "id": "TbLvCxxlFCn6"
   },
   "outputs": [],
   "source": [
    "en_sents_test = open('opus.en-ru-test.en').read().lower().splitlines()\n",
    "ru_sents_test = open('opus.en-ru-test.ru').read().lower().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "Dy0cRSoLFC0k",
   "metadata": {
    "id": "Dy0cRSoLFC0k"
   },
   "outputs": [],
   "source": [
    "translations = []\n",
    "\n",
    "for i in range(10):\n",
    "    translations.append(translate(en_sents_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "Cuj_lcoUFg9W",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cuj_lcoUFg9W",
    "outputId": "c5949743-1846-46b1-b758-e4c9471bc956"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['if you only stay there.',\n",
       " \"i don't know how you do it, pop, carrying these boxes around every day.\",\n",
       " 'we might have a slight edge in mediation.',\n",
       " 'how long is it going to take you to get him what he needs?',\n",
       " \"on 1 april president of the nagorno karabagh republic bako sahakyan met head of the general staff of the republic of armenia's armed forces colonel-general yuri khachaturov.\",\n",
       " 'mr priesner also noted that the e-justice management system has not only improved case management, but has also led to a significant streamlining in procedures.',\n",
       " \"you don't like chicken noodle soup?\",\n",
       " 'posted: 14 may 2005, 20:31',\n",
       " 'now, for a minute, i thought maybe he was being tailed.',\n",
       " '« : 26 октябрь 2017, 06:50:24 »']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sents_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "pRY7yBESGYxa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pRY7yBESGYxa",
    "outputId": "7fbaf103-1e48-41b3-f616-472821d34765"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['только бы не вылететь.',\n",
       " 'и как ты только справляешься, папа, таская эти коробки взад-вперед целый день.',\n",
       " 'возможно, у нас есть небольшое преимущество в переговорах.',\n",
       " 'сколько времени вы будете делать то, что ему нужно?',\n",
       " '1 апреля президент нкр бако саакян принял начальника генштаба вооруженных сил республики армения генерал-полковника юрия хачатурова.',\n",
       " 'г-н приснер также упомянул, что система электронного правосудия не только позволила улучшить процесс ведения дел, но также способствует значительному упорядочению процедур.',\n",
       " '- неплохо, да.',\n",
       " 'posted: 15 dec 2006, 00:07',\n",
       " 'и на минутку я подумал, что за ним могут следить.',\n",
       " '«: 11 октябрь 2011, 17:15:34»']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_sents_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YzWaYs0lIB80",
   "metadata": {
    "id": "YzWaYs0lIB80"
   },
   "source": [
    "BLEU очень сильно зависит от токенизации. В нашем случае это не проблема так как мы оцениваем одну модель. Но если бы мы хотели сравнить несколько, то нужно было бы создавать 1 общий токенизатор и рассчитывать через него"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "hx3kUhHQFhMQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hx3kUhHQFhMQ",
    "outputId": "82262875-3ef2-4232-caf3-36ad77f76320"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.8/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "bleus = []\n",
    "\n",
    "for i, t in enumerate(translations):\n",
    "    reference = tokenizer_ru.encode(t).tokens\n",
    "    hypothesis = tokenizer_ru.encode(ru_sents_test[i]).tokens\n",
    "\n",
    "    bleus.append(nltk.translate.bleu_score.sentence_bleu([reference], hypothesis,  auto_reweigh=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "rJDX-Z-3GKDE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rJDX-Z-3GKDE",
    "outputId": "ee22d497-8f53-4522-89c2-aeb5e329cbed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5141598784548078"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(bleus)/len(bleus))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Lwuh1mO8Icw0",
   "metadata": {
    "id": "Lwuh1mO8Icw0"
   },
   "source": [
    "BLEU обычно показывают не от 0 до 1, а от 0 до 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14270f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
